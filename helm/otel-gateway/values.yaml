# OpenTelemetry Collector Gateway
# Helm chart: open-telemetry/opentelemetry-collector
# Docs: https://opentelemetry.io/docs/collector/deployment/gateway/

image:
  repository: otel/opentelemetry-collector-contrib

mode: deployment
replicaCount: 3

resources:
  requests:
    cpu: "1"
    memory: "2Gi"
  limits:
    cpu: "2"
    memory: "4Gi"

autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 6
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: ScheduleAnyway
    labelSelector:
      matchLabels:
        app.kubernetes.io/name: opentelemetry-collector

service:
  type: ClusterIP

ports:
  otlp:
    enabled: true
    containerPort: 4317
    servicePort: 4317
    protocol: TCP
  otlp-http:
    enabled: true
    containerPort: 4318
    servicePort: 4318
    protocol: TCP
  health:
    enabled: true
    containerPort: 13133
    servicePort: 13133
    protocol: TCP

config:
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: "0.0.0.0:4317"
          max_recv_msg_size_mib: 8
        http:
          endpoint: "0.0.0.0:4318"

  processors:
    memory_limiter:
      check_interval: 1s
      limit_mib: 3072
      spike_limit_mib: 512

    batch:
      send_batch_size: 2000
      send_batch_max_size: 3000
      timeout: 2s

    # Tail-based sampling: keep all errors, all high-latency, 2% of normal
    tail_sampling:
      decision_wait: 10s
      num_traces: 100000
      expected_new_traces_per_sec: 1000
      policies:
        - name: errors-always
          type: status_code
          status_code:
            status_codes: [ERROR]
        - name: high-latency
          type: latency
          latency:
            threshold_ms: 1000
        - name: probabilistic-sample
          type: probabilistic
          probabilistic:
            sampling_percentage: 2

    # Drop health-check and readiness probe spans
    filter/healthcheck:
      error_mode: ignore
      traces:
        span:
          - 'attributes["http.route"] == "/health"'
          - 'attributes["http.route"] == "/healthz"'
          - 'attributes["http.route"] == "/ready"'
          - 'attributes["http.route"] == "/readyz"'
          - 'attributes["http.route"] == "/livez"'
          - 'attributes["http.target"] == "/health"'
          - 'attributes["url.path"] == "/health"'

    # Normalize attributes across different instrumentation libraries
    transform/normalize:
      error_mode: ignore
      trace_statements:
        - context: resource
          statements:
            - set(attributes["service.environment"], attributes["deployment.environment"]) where attributes["deployment.environment"] != nil

  exporters:
    otlphttp/mimir:
      endpoint: "http://mimir-distributor.observability.svc:8080/otlp"
      headers:
        X-Scope-OrgID: anonymous
      tls:
        insecure: true

    otlphttp/loki:
      endpoint: "http://loki-write.observability.svc:3100/otlp"
      tls:
        insecure: true

    otlphttp/tempo:
      endpoint: "http://tempo-distributor.observability.svc:4318"
      tls:
        insecure: true

  extensions:
    health_check:
      endpoint: "0.0.0.0:13133"

  service:
    extensions: [health_check]
    pipelines:
      metrics:
        receivers: [otlp]
        processors: [memory_limiter, transform/normalize, batch]
        exporters: [otlphttp/mimir]
      logs:
        receivers: [otlp]
        processors: [memory_limiter, transform/normalize, batch]
        exporters: [otlphttp/loki]
      traces:
        receivers: [otlp]
        processors: [memory_limiter, filter/healthcheck, tail_sampling, transform/normalize, batch]
        exporters: [otlphttp/tempo]
    telemetry:
      metrics:
        level: detailed
        readers:
          - pull:
              exporter:
                prometheus:
                  host: 0.0.0.0
                  port: 8888
